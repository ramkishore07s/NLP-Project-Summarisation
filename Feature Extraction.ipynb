{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Extraction** for **Extractive Summarisation**\n",
    "\n",
    "**Features Considered for each line:**\n",
    "* Number of Verbs\n",
    "* Number of Stop words\n",
    "* Number of Named Entities\n",
    "* Number of Pronouns\n",
    "* Position in document\n",
    "* Sentence Length\n",
    "\n",
    "** May include in Future:**\n",
    "* Discourse Cues\n",
    "* Sentiment\n",
    "* Salience\n",
    "* Uniqueness\n",
    "* Has Money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* Parsing raw data\n",
    "* POS tagging\n",
    "* Stop words\n",
    "* Count Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /home/ramkishore.s/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ramkishore.s/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN = os.listdir('Dataset/all/training/')\n",
    "DEV = os.listdir(\"Dataset/all/validation/\")\n",
    "TEST = os.listdir('Dataset/all/test/')\n",
    "TRAIN_FOLDER = 'Dataset/all/training/'\n",
    "DEV_FOLDER = 'Dataset/all/validation/'\n",
    "TEST_FOLDER = 'Dataset/all/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(277554, 11443, 13367)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRAIN), len(TEST), len(DEV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model dataset file:\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This dataset contains CNN and Dailymail articles used for training a summarization system. The script used to create the dataset is modified from the release of Hermann et al. 2015.\n",
    "\n",
    "### Format:\n",
    "\n",
    "Each file contains four parts separated by ‘\\n\\n’. They are\n",
    "* url of the original article;\n",
    "* sentences in the article and their labels (for sentence-based extractive summarization);\n",
    "* extractable highlights (for word extraction-based abstractive summarization);\n",
    "* named entity mapping.\n",
    "\n",
    "### Sentence labels\n",
    "\n",
    "There are three labels for the sentences: 1, 2 and 0. \n",
    "* ** 1 **—-sentence should extracted; \n",
    "* ** 2 **--sentence might be extracted; \n",
    "* ** 0 **—-sentence shouldn't be extracted.\n",
    "\n",
    "### Extractable highlights\n",
    "\n",
    "The extractable highlights are created by examining if a word (or its morphological transformation) in the highlight appears in the article or a general purpose stop-word list, which together constitute the output space (i.e., the allowed vocabulary during summary generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading all the Data to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entities = {}\n",
    "docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_parse(file):\n",
    "    global entities\n",
    "    # generating data\n",
    "    \n",
    "    contents = file.read()\n",
    "    parts = contents.split('\\n\\n')\n",
    "    lines = parts[1].split('\\n')\n",
    "    output = [int(line[-1]) for line in lines]\n",
    "    lines = [line[:-1].rstrip() for line in lines]\n",
    "    \n",
    "    # storing entities\n",
    "    entity_map = parts[3].split('\\n')\n",
    "    for i in entity_map:\n",
    "        id, name = i.split(\":\")[:2]\n",
    "        entities[id] = name\n",
    "    file.close()\n",
    "    return {\"lines\": lines, \"output\": output, \"summary\": parts[2].split('\\n')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_processed_files(folder, file_names):\n",
    "    docs = []\n",
    "    summaries = []\n",
    "    for file in file_names:\n",
    "        try:\n",
    "            docs.append(file_parse(open(folder + file)))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN = get_processed_files(TRAIN_FOLDER, TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST = get_processed_files(TEST_FOLDER, TEST)\n",
    "DEV = get_processed_files(DEV_FOLDER, DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_files(target_folder, string, docs, filename_pattern):\n",
    "    if not filename_pattern: filename_pattern = string\n",
    "    for doc, index in zip(docs, range(len(docs))):\n",
    "        with open(target_folder + filename_pattern + '.' + str(index) + '.txt', 'w+') as file:\n",
    "            for line in doc[string]:\n",
    "                file.write(str(line))\n",
    "                file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Dataset/Summaries/')\n",
    "os.mkdir('Dataset/Summaries/train/')\n",
    "os.mkdir('Dataset/Summaries/test')\n",
    "os.mkdir('Dataset/Summaries/dev/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_files('Dataset/Summaries/train/', 'summary', TRAIN)\n",
    "write_files('Dataset/Summaries/test/', 'summary', TEST)\n",
    "write_files('Dataset/Summaries/dev/', 'summary', DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Dataset/Text/')\n",
    "os.mkdir('Dataset/Text/train/')\n",
    "os.mkdir('Dataset/Text/test/')\n",
    "os.mkdir('Dataset/Text/dev/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_files('Dataset/Text/train/', 'lines', TRAIN, 'doc')\n",
    "write_files('Dataset/Text/test/', 'lines', TEST, 'doc')\n",
    "write_files('Dataset/Text/dev/', 'lines', DEV, 'doc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir('Dataset/Cheng_outputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir('Dataset/Cheng_outputs/train/')\n",
    "os.mkdir('Dataset/Cheng_outputs/test/')\n",
    "os.mkdir('Dataset/Cheng_outputs/dev/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_files('Dataset/Cheng_outputs/train/', 'output', TRAIN, 'output')\n",
    "write_files('Dataset/Cheng_outputs/test/', 'output', TEST, 'output')\n",
    "write_files('Dataset/Cheng_outputs/dev/', 'output', DEV, 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS data\n",
    "\n",
    "**NOTE: Since we only using primitive information like number of verbs, number of pronouns etc, we don't need accurate tags. So we simply use the most probable tag for each word. This reduces the computation time required for POS tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TAGS = {}\n",
    "CACHED_TAGS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache(word):\n",
    "    if word not in CACHED_TAGS:\n",
    "        CACHED_TAGS[word] = nltk.pos_tag([word])[0][1]\n",
    "    return CACHED_TAGS[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TAGS = []\n",
    "TEST_TAGS = []\n",
    "DEV_TAGS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_data(raw, final):\n",
    "    for doc in raw:\n",
    "        dic = []\n",
    "        for line in doc['lines']:\n",
    "            l = []\n",
    "            for word in line.split():\n",
    "                tag = cache(word)\n",
    "                l.append(tag)\n",
    "            dic.append(l)\n",
    "        final.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_data(TRAIN, TRAIN_TAGS)\n",
    "tag_data(TEST, TEST_TAGS)\n",
    "tag_data(DEV, DEV_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write(target_folder, filename_pattern, docs):\n",
    "    for doc, index in zip(docs, range(len(docs))):\n",
    "        with open(target_folder + filename_pattern + '.' + str(index) + '.txt', 'w+') as file:\n",
    "            for line in doc:\n",
    "                for tag in line:\n",
    "                    file.write(str(tag))\n",
    "                    file.write(' ')\n",
    "                file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Dataset/Tags')\n",
    "os.mkdir('Dataset/Tags/Train')\n",
    "os.mkdir('Dataset/Tags/Test')\n",
    "os.mkdir('Dataset/Tags/Dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "write('Dataset/Tags/Train/', 'tags', TRAIN_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write('Dataset/Tags/Test/', 'tags', TEST_TAGS)\n",
    "write('Dataset/Tags/Dev/', 'tags', DEV_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del TRAIN_TAGS, TEST_TAGS, DEV_TAGS, TAGS, CACHED_TAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_stops(data):\n",
    "    final = []\n",
    "    for doc in data:\n",
    "        stops = []\n",
    "        for line in doc['lines']:\n",
    "            count = 0\n",
    "            for word in line.split():\n",
    "                if word.lower() in STOPWORDS: count += 1\n",
    "            stops.append(count)\n",
    "        final.append(stops)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_STOPS = count_stops(TRAIN)\n",
    "TEST_STOPS = count_stops(TEST)\n",
    "DEV_STOPS = count_stops(DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Dataset/StopCounts/')\n",
    "os.mkdir('Dataset/StopCounts/train')\n",
    "os.mkdir('Dataset/StopCounts/test')\n",
    "os.mkdir('Dataset/StopCounts/dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write(target_folder, filename_pattern, docs):\n",
    "    for doc, index in zip(docs, range(len(docs))):\n",
    "        with open(target_folder + filename_pattern + '.' + str(index) + '.txt', 'w+') as file:\n",
    "            for line in doc:\n",
    "                file.write(str(line))\n",
    "                file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "write('Dataset/StopCounts/train/', 'stop_counts', TRAIN_STOPS)\n",
    "write('Dataset/StopCounts/test/', 'stop_counts', TEST_STOPS)\n",
    "write('Dataset/StopCounts/dev', 'stop_counts', DEV_STOPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del TRAIN_STOPS, TEST_STOPS, DEV_STOPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove 100 dimensional Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 400000 400000\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "word2id = {}\n",
    "vectors = []\n",
    "id = 0\n",
    "dims = 100\n",
    "glove_file = '/home/ramkishore.s/glove/glove.6B.100d.txt'\n",
    "with open(glove_file) as f:\n",
    "    for l in f:\n",
    "        line = l.split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2id[word] , id = id, id + 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "        \n",
    "print(len(words), len(word2id), len(vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random NER vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_names = ['@entity' + str(num) for num in range(0,1000)]\n",
    "entity2ids = {}\n",
    "i = len(words)\n",
    "for name in entity_names:\n",
    "    entity2ids[name], i = i, i + 1\n",
    "entity_vectors = np.random.randn(1000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words.extend(entity_names)\n",
    "word2id.update(entity2ids)\n",
    "vectors.extend(entity_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding **meta** vectors\n",
    "\n",
    "* `unknown`\n",
    "* `number`\n",
    "* `time`\n",
    "* `money`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UNK = '@@@UNK@@@'\n",
    "words.append(UNK)\n",
    "word2id[UNK] = len(words) - 1\n",
    "vectors.extend(np.random.randn(1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MONEY = '@@@MON@@@'\n",
    "TIME = '@@@TIME@@@'\n",
    "NUMBER = '@@@NUM@@@'\n",
    "words.extend([MONEY, TIME, NUMBER])\n",
    "word2id[MONEY], word2id[TIME], word2id[NUMBER] = len(words), len(words) + 1, len(words) + 2\n",
    "vectors.extend(np.random.randn(3, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(401004, 401004, 401004)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), len(vectors), len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2id[MONEY] = 401001\n",
    "word2id[TIME] = 401002\n",
    "word2id[NUMBER] = 401003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
