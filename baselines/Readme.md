## Logistic Regression and Naive Bayes

### <a href='https://docs.google.com/uc?id=0B0Obe9L1qtsnSXZEd0JCenIyejg&export=download'>Dataset</a>:
* We use the <a href='https://docs.google.com/uc?id=0B0Obe9L1qtsnSXZEd0JCenIyejg&export=download'>Dataset</a> given by <a href='https://aclweb.org/anthology/P16-1046'>Cheng et. al. 2016</a> which include several additions.

### **Modifications to the original data by <a href='https://aclweb.org/anthology/P16-1046'>Cheng et. al. 2016</a>.**
* NERed Data. All the named entities are replaced with unique tags.
* Truth labels for sentences to be extracted are generated using paralled corpora.

### Feature Extraction
* run the `Feature Extraction.ipynb`

**Features Considered for each line:**
* Number of Verbs (POS)
* Number of Stop words (nltk stop word list)
* Number of Named Entities
* Number of Pronouns (POS)
* Position in document 
* Sentence Length

**May include in Future:**
* Discourse Cues
* Sentiment
* Salience
* Uniqueness
* Has Money

### Helper functions
Run these two lines in IPython notebook to get all the features:
* `% run '<path to 'Get Features.ipynb>'`
* `pronoun, verbs, entity_counts, sen_len, sen_pos, stop_counts, output = get_all()
`

**NOTE: For baseline models we use truth labels generated by <a href='https://aclweb.org/anthology/P16-1046'>Cheng et. al. 2016</a>**
----
### Results on Cheng data: (scores mentioned are based on truth labels and not ROUGE)
#### Logistic Regression:
* Precision: 38%
* Recall: 85.6%
* F1 score: 52.8%
#### Naive Bayes:

-----

### To do
* ROUGE EVAL
